<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Portfolio Optimization on S&P 500 — Predicting Allocation Weights Directly</title>
    <style>
        body { font-family: Georgia, serif; max-width: 740px; margin: 2rem auto; padding: 0 1rem; line-height: 1.7; color: #222; }
        h1 { font-size: 2rem; }
        h2 { font-size: 1.5rem; margin-top: 2rem; }
        h3 { font-size: 1.25rem; }
        pre { background: #f5f5f5; padding: 1rem; overflow-x: auto; border-radius: 4px; }
        code { font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace; font-size: 0.9em; }
        table { border-collapse: collapse; width: 100%; margin: 1rem 0; }
        th, td { border: 1px solid #ddd; padding: 0.5rem 0.75rem; text-align: left; }
        th { background: #f5f5f5; }
        img { max-width: 100%; height: auto; display: block; margin: 1.5rem auto; }
        blockquote { border-left: 3px solid #ccc; margin: 1rem 0; padding-left: 1rem; color: #555; }
        hr { border: none; border-top: 1px solid #ddd; margin: 2rem 0; }
    </style>
</head>
<body>
<h1>Portfolio Optimization on S&amp;P 500 — Predicting Allocation Weights Directly</h1>
<p>In the <a href="https://medium.com/@alexandre.durand/portfolio-optimization-on-s-p-500-stocks-with-ml-predictions">previous article</a>, we trained ML models (Linear Regression, GBT) to <strong>predict quarterly returns</strong>, then fed those predictions into portfolio optimization. The whole pipeline was: predict returns → filter positive → optimize weights. Two separate stages. The model doesn't know what the optimizer will do with its output, and the optimizer doesnt care how the model was trained.</p>
<p>But is that really optimal? Minimizing MSE on individual stock returns is not the same as producing a good portfolio. A model can be great at predicting large-cap stable stocks (low variance, low alpha) and terrible at the volatile ones that actually drive portfolio performance. The optimizer then has to salvage whatever the model gives it.</p>
<p>In this article, we take a different route — <strong>we train a LightGBM model to predict the optimal portfolio weights directly</strong>. Instead of predicting returns and hoping the optimizer figures it out, we compute what the oracle-optimal Markowitz weights would have been (using hindsight), and train the model to replicate those weights from features. At inference, no optimizer needed — just a forward pass.</p>
<p>We also switch from sklearn's <code>GradientBoostingRegressor</code> to <strong>LightGBM</strong> — same idea, ~10x faster, and supports custom objectives which we'll need later.</p>
<hr />
<h2>Data &amp; Features (Same as Article 3)</h2>
<p>Same dataset — S&amp;P 500 constituents with 20 years of daily prices. Same 8 features:</p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Definition</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>ret_q1</code></td>
<td>Quarterly log return, lag 1</td>
</tr>
<tr>
<td><code>ret_q2</code></td>
<td>Quarterly log return, lag 2</td>
</tr>
<tr>
<td><code>ret_q4</code></td>
<td>Quarterly log return, lag 4</td>
</tr>
<tr>
<td><code>volatility_63d</code></td>
<td>Rolling 63-day std of daily log returns</td>
</tr>
<tr>
<td><code>volatility_252d</code></td>
<td>Rolling 252-day std of daily log returns</td>
</tr>
<tr>
<td><code>mom_12_1</code></td>
<td>12-month return minus last month return</td>
</tr>
<tr>
<td><code>volatility_ratio</code></td>
<td>Ratio of 63d / 252d volatility</td>
</tr>
<tr>
<td><code>mean_reversion</code></td>
<td>Price deviation from 252d moving average</td>
</tr>
</tbody>
</table>
<p>If you haven't read <a href="https://medium.com/@alexandre.durand/portfolio-optimization-on-s-p-500-stocks-with-ml-predictions">article 3</a>, check it for the feature definitions and train/valid/test split logic. Same temporal split: 70/15/15.</p>
<pre><code class="language-python">df = pd.read_csv('../data/sp500_20years.csv')

df['Log_Return'] = df.groupby('Ticker')['Adj Close'].transform(lambda x: np.log(x / x.shift(1)))

days = 63
df['Quarterly_Log_Return'] = df.groupby('Ticker')['Log_Return']\
    .rolling(window=days, min_periods=days).sum()\
    .reset_index(0, drop=True)

feature_cols = ['ret_q1','ret_q2','ret_q4','volatility_63d',
                'volatility_252d','mom_12_1','volatility_ratio','mean_reversion']
df['target'] = df.groupby('Ticker')['Quarterly_Log_Return'].shift(-days)
</code></pre>
<p>Output:</p>
<pre><code>Total rows: 2,518,880
train :  1411253 rows | 2006-02-24 -&gt; 2020-02-14
valid :   379262 rows | 2020-02-18 -&gt; 2023-02-13
test  :   379765 rows | 2023-02-14 -&gt; 2026-02-18
test rebalancing dates : 12
</code></pre>
<p>We also prepare quarterly-sampled versions of train and valid (one observation per 63 trading days). This gives non-overlapping returns for the weight model:</p>
<pre><code>Quarterly train : 22295 rows | 56 dates
Quarterly valid :  5848 rows | 14 dates
</code></pre>
<p>So ~22K rows for the weight model vs ~1.4M for the return model. Big tradeoff.</p>
<hr />
<h2>LightGBM Baseline — MSE Loss</h2>
<p>First, a standard LightGBM regressor trained on the full daily dataset. This replaces <code>GradientBoostingRegressor</code> from article 3.</p>
<pre><code class="language-python">params_mse = {
    'objective': 'regression',
    'metric': 'rmse',
    'learning_rate': 0.05,
    'num_leaves': 31,
    'max_depth': 4,
    'subsample': 0.7,
    'colsample_bytree': 0.8,
    'min_child_samples': 50,
    'random_state': 42,
    'verbose': -1,
}

lgbm_mse = lgb.train(
    params_mse, dtrain_mse,
    num_boost_round=500,
    valid_sets=[dvalid_mse],
    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(50)]
)
</code></pre>
<p>Best iteration stops around 200 rounds. The RMSE on validation is comparable to what we got with sklearn's GBR. Feature importance shows <code>volatility_ratio</code> and <code>volatility_252d</code> dominating, similar pattern to article 3.</p>
<hr />
<h2>Direct Weight Prediction — The Idea</h2>
<p>The standard approach (articles 2 &amp; 3): predict returns r̂ᵢ, then optimize weights wᵢ given r̂ᵢ and a covariance matrix Σ. Two separate stages that don't talk to each other.</p>
<p>The idea here: <strong>skip the two-stage pipeline entirely</strong>. If we can learn what good allocations look like from historical data, we don't need the optimizer at test time.</p>
<h3>Oracle Weight Targets</h3>
<p>For each rebalancing date <em>t</em> in the training set, we have access to the realised future returns rᵢ⁽ᵗ⁾ (hindsight — these are the actual returns that happened). We run Markowitz optimization on these realised returns to get the oracle-optimal weights:</p>
<blockquote>
<p><strong>w*(t) = argmin_w [ γ · wᵀΣw − wᵀr(t) ]</strong></p>
</blockquote>
<p>subject to Σwᵢ = 1 and 0 ≤ wᵢ ≤ 0.3</p>
<p>These w*ᵢ⁽ᵗ⁾ become training targets. The model learns:</p>
<blockquote>
<p><strong>f(featuresᵢ⁽ᵗ⁾) ≈ wᵢ*⁽ᵗ⁾</strong></p>
</blockquote>
<h3>Why This Can Work</h3>
<p>The key assumption: <strong>stocks that deserved high weight in the past share recognisable feature patterns</strong>. If volatility, momentum, and past returns carry predictive signal for which stocks will end up in the optimal portfolio, the model can learn that mapping without ever predicting returns explicitly.</p>
<p>Think of it this way — the oracle weights encode everything the optimizer would have done with perfect information. Instead of learning returns and then optimizing, we learn the end result directly.</p>
<h3>Inference</h3>
<p>At test time, the model predicts a raw weight ŵᵢ for each stock. We apply:</p>
<ol>
<li>Floor negative predictions: ŵᵢ = max(ŵᵢ, 0)</li>
<li>Normalize: ŵᵢ = ŵᵢ / Σⱼ ŵⱼ</li>
</ol>
<p>No optimizer needed. Just clip and normalize.</p>
<hr />
<h2>Computing Oracle Weights</h2>
<p>This part is computationally heavier than training the model. For each of the 56 quarterly training dates, we run Markowitz optimization on ~400 stocks using realized returns:</p>
<pre><code class="language-python">def compute_oracle_weights(df_q, cov_matrix, alloc_fun=neg_markowitz_objective):
    weights = pd.Series(0.0, index=df_q.index)

    for date, group in df_q.groupby('Date'):
        tickers = group['Ticker'].values
        returns = group['target'].values

        # Filter: in covariance matrix, non-NaN, positive return
        in_cov = np.isin(tickers, cov_matrix.columns)
        valid = in_cov &amp; ~np.isnan(returns)
        t_v = tickers[valid]
        r_v = returns[valid]

        pos = r_v &gt; 0
        if pos.sum() &lt; 5:
            continue

        t_pos = t_v[pos]
        r_pos = r_v[pos]
        cov_sub = cov_matrix.loc[t_pos, t_pos].values

        w = optimize_weights(r_pos, cov_sub, fun=alloc_fun)
        weights.loc[group.index[valid][pos]] = w

    return weights.values
</code></pre>
<p>Important nuance: most stocks get <strong>zero weight</strong> in the oracle allocation. The Markowitz optimizer concentrates on a handful of stocks. So the target distribution is heavily skewed — lots of zeros, a few positive values around 0.01–0.30. This makes regression harder than predicting returns, where the target distribution is roughly symmetric.</p>
<pre><code>Non-zero training weights: 3251 / 22295 (14.6%)
Non-zero validation weights: 842 / 5848 (14.4%)
Mean non-zero weight (train): 0.0408
</code></pre>
<p>Only ~15% of training samples have non-zero target weight. The model needs to learn both <strong>which stocks to include</strong> and <strong>how much to allocate</strong>.</p>
<hr />
<h2>Training the Weight Model</h2>
<pre><code class="language-python">params_weight = {
    'objective': 'regression',
    'metric': 'rmse',
    'learning_rate': 0.01,
    'num_leaves': 15,
    'max_depth': 3,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'min_child_samples': 30,
    'random_state': 42,
    'verbose': -1,
}

lgbm_weights = lgb.train(
    params_weight, dtrain_w,
    num_boost_round=1000,
    valid_sets=[dvalid_w],
    callbacks=[lgb.early_stopping(100), lgb.log_evaluation(100)]
)
</code></pre>
<p>We use a lower learning rate and shallower trees compared to the MSE model — the weight targets are noisier (oracle weights are sensitive to small changes in realized returns).</p>
<h3>Feature Importance Comparison</h3>
<p><img alt="Feature Importance — LGBM-MSE vs LGBM-Weights" src="https://raw.githubusercontent.com/alexandreib/medium/main/articles/images/article4_feature_importance_comparison.png" /></p>
<p>The two models learn different feature dependencies. LGBM-MSE (return prediction, left) spreads importance more evenly. The weight model (right) tends to weight volatility and momentum features differently — it's learning "which stocks belong in a Markowitz-optimal portfolio" rather than "which stocks will have high returns". These are related but not identical tasks.</p>
<hr />
<h2>Prediction Comparison</h2>
<p>The two models predict fundamentaly different things — returns vs. weights. To compare allocations fairly, we convert MSE predictions to softmax weights:</p>
<p><img alt="Prediction Comparison — MSE vs Weights" src="https://raw.githubusercontent.com/alexandreib/medium/main/articles/images/article4_prediction_comparison.png" /></p>
<p>Left: raw prediction distributions. The return model outputs values around ±0.15 (log returns), while the weight model outputs values clustered near zero with a positive tail (weights). Middle: after normalizing both to allocation weights, the scatter shows moderate correlation — the models agree on some stocks but disagree on others. Right: Spearman rank correlation per rebalancing date varies between ~0.2 and ~0.6.</p>
<p>The models don't agree perfectly, and they shouldn't — they're answering different questions.</p>
<hr />
<h2>Backtest</h2>
<p>Same framework as article 3, with one addition. For each rebalancing date:</p>
<ol>
<li><strong>Momentum and LGBM-MSE</strong>: predict → filter positive → optimize with 4 methods (random, max Sharpe, min variance, Markowitz)</li>
<li><strong>LGBM-Weights</strong>: predict weights directly → clip negatives → normalize → done</li>
</ol>
<p>This gives us 2×4 + 1 = 9 strategies. The weight model's whole point is that it skips optimization.</p>
<pre><code class="language-python"># LGBM-Weights: direct allocation (no optimizer)
w_pred = p_weights.loc[common_w].values
w_pred = np.maximum(w_pred, 0)
if w_pred.sum() &gt; 1e-8:
    w_pred = w_pred / w_pred.sum()
    results[date]['returns_direct_lgbm_weights'] = (w_pred * future_w).sum()
</code></pre>
<hr />
<h2>Results</h2>
<p><img alt="Cumulative Quarterly Log Returns — All Strategies" src="https://raw.githubusercontent.com/alexandreib/medium/main/articles/images/article4_cumulative_returns.png" /></p>
<p>The <code>mv_momentum</code> strategy dominates on raw return (1.47 cumulative log return), same as article 3. <code>direct_lgbm_weights</code> reaches 0.36 — second highest cumulative. The min-variance strategies cluster tightly around 0.21–0.24.</p>
<p>Full comparison:</p>
<table>
<thead>
<tr>
<th>Strategy</th>
<th>Total Return</th>
<th>Avg Q Return</th>
<th>Std Q Return</th>
<th>Realised Sharpe</th>
<th>Max Q Drawdown</th>
<th>Best Q Return</th>
</tr>
</thead>
<tbody>
<tr>
<td>mv_momentum</td>
<td>1.4663</td>
<td>0.1333</td>
<td>0.1807</td>
<td>0.7378</td>
<td>-0.1659</td>
<td>0.3669</td>
</tr>
<tr>
<td>minvar_lgbm_mse</td>
<td>0.2373</td>
<td>0.0216</td>
<td>0.0366</td>
<td>0.5891</td>
<td>-0.0199</td>
<td>0.0782</td>
</tr>
<tr>
<td>random_lgbm_mse</td>
<td>0.2210</td>
<td>0.0201</td>
<td>0.0476</td>
<td>0.4218</td>
<td>-0.0627</td>
<td>0.1077</td>
</tr>
<tr>
<td>random_momentum</td>
<td>0.2218</td>
<td>0.0202</td>
<td>0.0491</td>
<td>0.4110</td>
<td>-0.0524</td>
<td>0.1233</td>
</tr>
<tr>
<td>direct_lgbm_weights</td>
<td>0.3639</td>
<td>0.0331</td>
<td>0.0816</td>
<td>0.4056</td>
<td>-0.0815</td>
<td>0.1986</td>
</tr>
<tr>
<td>minvar_momentum</td>
<td>0.2133</td>
<td>0.0194</td>
<td>0.0567</td>
<td>0.3421</td>
<td>-0.0488</td>
<td>0.1217</td>
</tr>
<tr>
<td>mv_lgbm_mse</td>
<td>0.2535</td>
<td>0.0230</td>
<td>0.0723</td>
<td>0.3189</td>
<td>-0.0870</td>
<td>0.1385</td>
</tr>
<tr>
<td>sharpe_momentum</td>
<td>0.2006</td>
<td>0.0182</td>
<td>0.0793</td>
<td>0.2300</td>
<td>-0.1058</td>
<td>0.1255</td>
</tr>
<tr>
<td>sharpe_lgbm_mse</td>
<td>0.1240</td>
<td>0.0113</td>
<td>0.1674</td>
<td>0.0673</td>
<td>-0.1740</td>
<td>0.3082</td>
</tr>
</tbody>
</table>
<p><img alt="Total Return and Realised Sharpe — Bar Charts" src="https://raw.githubusercontent.com/alexandreib/medium/main/articles/images/article4_total_return_sharpe.png" /></p>
<p>The direct weight model (0.36 total return, 0.41 Sharpe) <strong>outperforms all strategies on cumulative return except <code>mv_momentum</code></strong>. On Sharpe ratio, it lands in the middle of the pack — beaten by <code>minvar_lgbm_mse</code> (0.59 Sharpe) which has much lower volatility but also lower return.</p>
<h3>Per-Allocation Comparisons</h3>
<p><img alt="SHARPE Allocation: Comparison" src="https://raw.githubusercontent.com/alexandreib/medium/main/articles/images/article4_sharpe_comparison.png" /></p>
<p><img alt="MINVAR Allocation: Comparison" src="https://raw.githubusercontent.com/alexandreib/medium/main/articles/images/article4_minvar_comparison.png" /></p>
<p><img alt="MV Allocation: Comparison" src="https://raw.githubusercontent.com/alexandreib/medium/main/articles/images/article4_mv_comparison.png" /></p>
<p><img alt="RANDOM Allocation: Comparison" src="https://raw.githubusercontent.com/alexandreib/medium/main/articles/images/article4_random_comparison.png" /></p>
<p>The purple dotted line (direct weight model) is plotted as reference on each chart. It consistently beats random allocation and sits between min-variance and the more aggressive strategies.</p>
<hr />
<h2>Comparison with Article 3</h2>
<p>For reference — the top strategies from article 3 (sklearn GBR, Linear Regression):</p>
<table>
<thead>
<tr>
<th>Strategy</th>
<th>Total Return</th>
<th>Realised Sharpe</th>
</tr>
</thead>
<tbody>
<tr>
<td>mv_momentum</td>
<td>1.47</td>
<td>0.74</td>
</tr>
<tr>
<td>minvar_gbr</td>
<td>0.23</td>
<td>0.63</td>
</tr>
<tr>
<td>minvar_lr</td>
<td>0.23</td>
<td>0.55</td>
</tr>
<tr>
<td>sharpe_lr</td>
<td>0.34</td>
<td>0.54</td>
</tr>
</tbody>
</table>
<p>The LGBM-MSE results are very close to sklearn GBR — same algorithm, slightly different implementation. The direct weight model (0.36 return, 0.41 Sharpe) would rank 3rd on total return in article 3's table, beating <code>sharpe_lr</code> and all <code>minvar_*</code> on return while using no optimizer.</p>
<hr />
<h2>Discussion</h2>
<p>The direct weight prediction model learns a fundamentaly different task. Instead of predicting individual stock returns and hoping the optimizer converts them into a good portfolio, it directly learns what a good allocation looks like from historical optimal portfolios.</p>
<p>The feature importance differences reflct this: the weight model may emphasize features that predict "portfolio fit" (volatility features that identify diversifiers) rather than features that predict raw returns (momentum signals).</p>
<p>The main tradeoff is data efficiency. The standard pipeline benefits from 1.4M training rows. The weight model trains on ~22K quarterly rows because it needs non-overlapping returns to compute meaningful oracle weights. Less data, but a more direct learning signal.</p>
<p>Another nuance: the direct weight model produces one fixed allocation per rebalancing date, without the flexibility of choosing different optimizers at test time. The MSE model can be paired with any allocation method — max Sharpe, min variance, etc.</p>
<h3>Why Mean-Variance with Momentum Dominates</h3>
<p>The one strategy that crushes everything is <code>mv_momentum</code>. Mean-variance optimization with momentum predictions benefits from the trending nature of the test period:</p>
<ul>
<li><strong>Momentum signals are clean when trends persist.</strong> In a sustained bull run, last quarter's winners keep winning. The return spread is large, so the μ̂ fed to the optimizer is accurate and well-separated from noise.</li>
<li><strong>The optimizer amplifies correct signals.</strong> Mean-variance concentrates on high-predicted-return stocks while diversifying risk. When predictions are directionally right, this amplification produces outsized returns.</li>
<li><strong>Volatility structure is stable in trends.</strong> Covariance estimated from recent data reflects the regime well. Correlations don't spike randomly like in crashes, so risk estimates remain useful.</li>
<li><strong>The 30% cap prevents worst-case overconcentration</strong> but still allows heavy bets on top picks — exactly what works when momentum is correct.</li>
</ul>
<p>However, mean-variance is fragile in non-trending markets. When expected returns μ ≈ 0 for most stocks, the optimizer chases noise: tiny estimation errors get amplified into large weight swings, turnover spikes, and the portfolio degrades to a noisy version of min-variance. The direct weight prediction approach should be more robust in those regimes because it learns a stable mapping from features to allocations rather than relying on a point estimate of μ.</p>
<hr />
<h2>Conclusion</h2>
<p>Predicting portfolio weights directly is conceptualy simpler than the two-stage pipeline: no optimizer at inference, just a forward pass. The model achieves the second highest total return (0.36) while requiring zero post-hoc optimization.</p>
<p><strong>Key takeaways:</strong> The weight model captures different feature relationships than the return model. Data efficiency is the main bottleneck — quarterly sampling limits training data. The direct approach trades flexibility for simplicity. Mean-variance dominates in trendy markets but is the most vulnerable to regime changes. The ideal next step is probably regime detection — use direct weights in choppy markets, mean-variance in trending ones.</p>
<h2>Limitations</h2>
<ul>
<li>Oracle weights are computed with hindsight — the model learns from an idealised target</li>
<li>Quarterly sampling reduces training data (~22K vs 1.4M)</li>
<li>Target weights are mostly zero (skewed distribution), making regression harder</li>
<li>Survivorship bias, static covariance, no transaction costs (same as articles 2-3)</li>
<li>The model learns one allocation style (Markowitz) — can't switch at test time</li>
<li>Test period is predominantly trending — results may not generalise to bear/sideways markets</li>
</ul>
<h2>Next Steps</h2>
<ul>
<li>Train separate weight models for each allocation method (max Sharpe, min variance)</li>
<li>Multi-task learning: predict both returns and weights simultaneously</li>
<li>Weighted loss: penalise errors on non-zero weights more heavily</li>
<li>Ensemble: average direct-weight and optimizer-based allocations</li>
<li>Transaction cost penalty on weight changes between periods</li>
<li>Regime detection: direct weights in choppy markets, mean-variance in trending ones</li>
</ul>
<hr />
<p><strong>Full Notebook / Code available :</strong></p>
<p><a href="https://github.com/alexandreib/medium/blob/main/notebooks/4_SP500_Portfolio_Sharpe_Optimization.ipynb">https://github.com/alexandreib/medium/blob/main/notebooks/4_SP500_Portfolio_Sharpe_Optimization.ipynb</a></p>
</body>
</html>
